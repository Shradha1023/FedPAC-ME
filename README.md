# Federated Learning Framework for Medical Image Analysis with Perspective-Aware Contrastive and Mixture of Experts (FedPAC-ME)
This repository contains the official implementation of FedPAC-ME, a federated learning framework that integrates Multi-Perspective Contrastive Learning (MPCL) with a Mixture-of-Experts (ME) personalization module to achieve robust, privacy-preserving medical image segmentation across heterogeneous clients.

The code supports federated training with labeled and unlabeled data, multi-perspective augmentations, contrastive alignment, and client-specific expert routing. It is built for reproducibility and fully aligned with the code policies of Nature Scientific Reports.

# ğŸ§  Overview
This repository contains the source code used to implement the FedPAC-ME framework described in the associated manuscript. The framework integrates (i) multi-perspective contrastive learning for representation alignment and (ii) a mixture-of-experts module for client-specific personalization within a federated learning environment. The codebase supports semi-supervised training using both labeled and unlabeled medical imaging data and is designed to operate under heterogeneous (non-IID) client distributions.

All experiments presented in the manuscript can be reproduced using the scripts and configuration files provided here.

# ğŸ“ Repository Structure
````
ğŸ“¦ FedPAC-ME/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE   
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ version1_8.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ download.py
â”‚   â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ visualization/
â”‚   â”‚   â”œâ”€â”€ visualize_slices.py
â”‚   â”‚   â”œâ”€â”€ plot_modalities.py
â”‚   â”‚   â”œâ”€â”€ histogram_plots.py
â”‚   â”‚   â””â”€â”€ segmentation_plots.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ unet.py
â”‚   â”‚   â””â”€â”€ other_models.py
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â””â”€â”€ federated/
â”‚       â”œâ”€â”€ split_clients.py
â”‚       â”œâ”€â”€ fedavg.py
â”‚       â”œâ”€â”€ utils.py
â”‚       â””â”€â”€ simulation.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_preprocessing.sh
â”‚   â”œâ”€â”€ run_training.sh
â”‚   â””â”€â”€ run_visualization.sh
â”‚
â””â”€â”€ results/
    â”œâ”€â”€ sample_plots/
    â”œâ”€â”€ models/
    â””â”€â”€ logs/

````

# âš™ï¸ Installation
## ğŸš€ Environment Setup

````

- Python â‰¥ 3.8  
- PyTorch â‰¥ 1.12  

```bash
git clone https://github.com/yourusername/FedPAC-ME.git
cd FedPAC-ME
python -m venv fedpacme_env
source fedpacme_env/bin/activate      # Linux/macOS  
fedpacme_env\Scripts\activate         # Windows
pip install -r requirements.txt

````

## ğŸ—‚ï¸ Dataset Preparation
Datasets are not included in this repository. Users should:

Download the dataset(s) used in the manuscript
Place the files under the data/ directory or specify alternative paths in configs/data_config.yaml.
If federated partitioning is required, specify the number of clients, labeled/unlabeled ratios, and distribution settings in the YAML configuration.
Running Experiments
Federated Training
To reproduce the main experimental results:
```
python src/main.py --config configs/fed_config.yaml
```
# ğŸ“Š Evaluation Procedures
To evaluate a trained global model:
```
python src/evaluation/eval.py --checkpoint path/to/model.pth
```
# ğŸ” Reproducibility
All random seeds used in the manuscript can be set via command-line arguments or within the configuration files. To reproduce the exact experimental conditions:
```
python src/main.py --config configs/fed_config.yaml --seed 42
```
# ğŸ“Œ Citation
Please cite the associated manuscript when using this repository:

```
Federated Learning Framework for Medical Image Analysis with Perspective-Aware Contrastive and Mixture of Experts
K. Hemalatha, Shradhanjali Das
Scientific Reports (Nature)
DOI: To be announced
```

# ğŸ“œ License
This repository is intended for **academic and research use only**.

Please see the `LICENSE` file for details.





